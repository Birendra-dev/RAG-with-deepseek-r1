{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "1. Use the following pieces of context to answer the question at the end.\n",
      "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\n",
      "\n",
      "3. Keep the answer crisp and limited to 3,4 sentences.\n",
      "Context: Context:\n",
      "content:Birendra Sharma birendrasharma0226@gmail.com\n",
      "ML Intern 9817788903\n",
      "A dedicated and driven student eager to join the workforce and gain lahan-24, lahan, Nepal\n",
      "practical experience in Machine Learning. Proven ability to complete\n",
      "tasks efficiently, both independently and collaboratively.\n",
      "source:Resume.pdf\n",
      "\n",
      "Context:\n",
      "content:Dependable, @https://x.com/ardnerib4\n",
      "reliable, and committed to learning and growing while contributing to\n",
      "the success of your organization. github.com/birendra-dev\n",
      "medium.com/@birendrasharma0226\n",
      "EDUCATION SKILLS\n",
      "Bachelor of Computer Engineering\n",
      "Python Flask Django Pandas\n",
      "Tribhuvan University\n",
      "Machine Learning Fine Tuning\n",
      "03/2021 - Present,\n",
      "Computer Skills Backend developer\n",
      "PERSONAL PROJECTS\n",
      "Sticky Notes STRENGTHS\n",
      "A web application developed using Django, HTML, and CSS that allows\n",
      "users to post blogs and share stories related to Machine Learning and\n",
      "Python Programming\n",
      "Data Science topics. Proficient in Python and its related libraries like NumPy, Matplotlib,\n",
      "ChatQwen and Pandas\n",
      "A web application built with Streamlit, serving as a personal ChatGPT for\n",
      "Web Development\n",
      "answering basic coding and general queries. It leverages an LLM model\n",
      "fine-tuned on a Python queries dataset to provide accurate and context- Skilled in frontend development using HTML, CSS and backend\n",
      "specific responses. development using Flask, Django and MySQL. Heart Disease Prediction Data Analysis\n",
      "A project developed using Tkinter, scikit-learn, pandas, and Matplotlib to Completed online courses in machine learning, data analysis, and\n",
      "predict whether a patient has heart disease based on their current pandas, showcasing proactive learning approach. medical details. Project Management\n",
      "Demonstrated practical application of skills through successful\n",
      "project implementations, such as Crop and Fertilizer\n",
      "Recommendation System, MCQs Generation, Sticky notes and\n",
      "PROJECTS\n",
      "ChatQwen. Crop and Fertilizer Recommendation System\n",
      "Purwanchal campus\n",
      "CERTIFICATES\n",
      "Description of Project\n",
      "An ML-powered web application developed using Flask, HTML,\n",
      "Supervised Machine Learning: Regression and\n",
      "CSS, scikit-learn, pandas, and MySQL. Users can input specific\n",
      "Classification\n",
      "data, and the system provides recommendations for suitable\n",
      "crops and fertilizers based on the input parameters. Unsupervised Learning, Recommenders,\n",
      "Reinforcement Learning\n",
      "MCQs Generation using NLP\n",
      "Purwanchal campus Advanced Learning Algorithms\n",
      "Description of Project\n",
      "ASSOCIATE DATA SCIENTIST by DataCamp\n",
      "An NLP project developed using Django, Transformers, and a\n",
      "fine-tuned LLM. This web application allows users to input text Learn Data Analysis With Pandas 2024\n",
      "and generate multiple-choice questions (MCQs) along with\n",
      "distractors based on the provided content. INTERESTS\n",
      "Programming Data Science AI LLM\n",
      "Machine Learning Technology\n",
      "\n",
      "source:Resume.pdf\n",
      "Question: is birendra a intern or full time worker ?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response:\n",
      "<think>\n",
      "Okay, so I need to figure out if Birendra Sharma is an intern or a full-time worker based on the provided context. Let me go through the information step by step.\n",
      "\n",
      "First, looking at the first block of text, it mentions \"ML Intern 9817788903.\" That part clearly states that Birendra is an intern with the number 9817788903. So that's a strong indicator he's still working part-time.\n",
      "\n",
      "The second block adds more context about his experience. It says \"A dedicated and driven student eager to join the workforce and gain lhan-24, lahan, Nepal.\" The word \"student\" here is interesting because it makes me think he might be studying for university or an advanced degree. If he's a student, that usually means part-time work.\n",
      "\n",
      "Additionally, the resume mentions practical experience in Machine Learning with proven skills. This experience could indicate some professional duties beyond just being an intern, like handling projects and contributing to teams.\n",
      "\n",
      "The education background is a Bachelor of Computer Engineering, which shows he has higher education but doesn't specify if it's full-time or part-time. However, the initial mention of being a student in this block suggests part-time.\n",
      "\n",
      "Looking at his projects on Django, Streamlit, etc., these are web applications, which often require daily tasks and contribute to larger projects. This reinforces that he's doing some work beyond just intern duties.\n",
      "\n",
      "Personal projects like the web application for Machine Learning topics also show active involvement in developing features and collaborating with others. These activities typically involve part-time roles.\n",
      "\n",
      "So putting it all together: Birendra is clearly an intern, but the context also mentions him as a dedicated student. However, if we're strictly looking at job titles and the immediate words, he's more likely an intern. The other information doesn't change that.\n",
      "</think>\n",
      "\n",
      "Birendra is identified as an ML Intern due to the \"ML Intern 9817788903\" designation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load the PDF\n",
    "pdf_path = \"sm.pdf\"  # Ensure this file exists before running the script\n",
    "loader = PDFPlumberLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Instantiate the embedding model\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = SemanticChunker(embedder)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create the vector store and fill it with embeddings\n",
    "vector = FAISS.from_documents(documents, embedder)\n",
    "retriever = vector.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Define llm\n",
    "llm = Ollama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "1. Use the following pieces of context to answer the question at the end.\n",
    "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\\n\n",
    "3. Keep the answer crisp and limited to 3,4 sentences.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=QA_CHAIN_PROMPT,\n",
    "    callbacks=None,\n",
    "    verbose=True)\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\", \"source\"],\n",
    "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
    ")\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=document_prompt,\n",
    "    callbacks=None)\n",
    "\n",
    "qa = RetrievalQA(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    verbose=True,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True)\n",
    "\n",
    "# User input\n",
    "user_input = input(\"Ask a question related to the PDF: \")\n",
    "\n",
    "# Process user input\n",
    "if user_input:\n",
    "    response = qa(user_input)[\"result\"]\n",
    "    print(\"Response:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # python\n",
    "# import os\n",
    "\n",
    "# def load_documents(directory):\n",
    "#     documents = []\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith('.txt'):\n",
    "#             with open(os.path.join(directory, filename), 'r') as file:\n",
    "#                 documents.append(file.read())\n",
    "#     return documents\n",
    "\n",
    "# documents = load_documents('files')\n",
    "# # # python\n",
    "# # %pip install faiss-cpu\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize the embeddings model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Function to generate embeddings\n",
    "# def embed(text):\n",
    "# \tinputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "# \twith torch.no_grad():\n",
    "# \t\toutputs = model(**inputs)\n",
    "# \treturn outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# # Generate embeddings for all documents\n",
    "# document_embeddings = [embed(doc) for doc in documents]\n",
    "# document_embeddings = np.array(document_embeddings).astype('float32')\n",
    "\n",
    "# # Create FAISS index\n",
    "# index = faiss.IndexFlatL2(document_embeddings.shape[1])  # L2 distance metric\n",
    "# index.add(document_embeddings)  # Add document embeddings to the index\n",
    "# # python\n",
    "# class SimpleRetriever:\n",
    "#     def __init__(self, index, embed_function):\n",
    "#         self.index = index\n",
    "#         self.embed_function = embed_function\n",
    "    \n",
    "#     def retrieve(self, query, k=3):\n",
    "#         query_embedding = self.embed_function(query)\n",
    "#         distances, indices = self.index.search(np.array([query_embedding]).astype('float32'), k)\n",
    "#         return [documents[i] for i in indices[0]]\n",
    "\n",
    "# retriever = SimpleRetriever(index, embed)\n",
    "# # python\n",
    "# from langchain_ollama import OllamaLLM\n",
    "# from string import Template\n",
    "\n",
    "# # Instantiate the model\n",
    "# llm = OllamaLLM(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# # Craft the prompt template using string. Template for better readability\n",
    "# prompt_template = Template(\"\"\"\n",
    "# Use ONLY the context below.\n",
    "# If unsure, say \"I don't know\".\n",
    "# Keep answers under 4 sentences.\n",
    "\n",
    "# Context: $context\n",
    "# Question: $question\n",
    "# Answer:\n",
    "# \"\"\")\n",
    "\n",
    "# # python\n",
    "# def answer_query(question):\n",
    "#     # Retrieve relevant context from the knowledge base\n",
    "#     context = retriever.retrieve(question)\n",
    "    \n",
    "#     # Combine retrieved contexts into a single string (if multiple)\n",
    "#     combined_context = \"n\".join(context)\n",
    "    \n",
    "#     # Generate an answer using DeepSeek R1 with the combined context\n",
    "#     response = llm.generate(prompt_template.substitute(context=combined_context, question=question))\n",
    "    \n",
    "#     return response.strip()\n",
    "# # python\n",
    "# if __name__ == \"__main__\":\n",
    "#     user_question = \"What are the key features of DeepSeek R1?\"\n",
    "#     answer = answer_query(user_question)\n",
    "#     print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
